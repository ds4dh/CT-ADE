{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c416eb-886d-470e-a91c-2fe1c7bce7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "original_sys_path = sys.path.copy()\n",
    "try:\n",
    "    sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "    from src.modeling_helpers import (\n",
    "        MultilabelModel,\n",
    "        AutoTokenizer,\n",
    "        load_trial_data,\n",
    "        CustomDataset,\n",
    "        custom_collate_fn,\n",
    "    )\n",
    "finally:\n",
    "    sys.path = original_sys_path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "\n",
    "def mcnemar_test(true_labels, preds_model_1, preds_model_2, exact):\n",
    "    \"\"\"\n",
    "    Performs McNemar's test to compare the performance of two binary predictive models\n",
    "    separately for positive instances, negative instances, and all instances.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: The true labels of the data.\n",
    "    - preds_model_1: The predictions made by the first model.\n",
    "    - preds_model_2: The predictions made by the second model.\n",
    "    - exact: If exact test should be used.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the McNemar Test Statistic, the p-value, and the contingency table\n",
    "      for positive instances, negative instances, and all instances.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Define a local function to calculate the contingency table and perform the test\n",
    "    def perform_test(true_labels_subset, preds_model_1_subset, preds_model_2_subset):\n",
    "        good_preds_model_1 = (preds_model_1_subset == true_labels_subset).astype(int)\n",
    "        good_preds_model_2 = (preds_model_2_subset == true_labels_subset).astype(int)\n",
    "\n",
    "        a, b, c, d = 0, 0, 0, 0\n",
    "        for i in range(len(good_preds_model_1)):\n",
    "            if good_preds_model_1[i] == 1 and good_preds_model_2[i] == 1:\n",
    "                a += 1\n",
    "            elif good_preds_model_1[i] == 1 and good_preds_model_2[i] == 0:\n",
    "                b += 1\n",
    "            elif good_preds_model_1[i] == 0 and good_preds_model_2[i] == 1:\n",
    "                c += 1\n",
    "            elif good_preds_model_1[i] == 0 and good_preds_model_2[i] == 0:\n",
    "                d += 1\n",
    "\n",
    "        contingency_table = np.array([[a, b], [c, d]])\n",
    "        mcnemar_result = mcnemar(contingency_table, exact=exact)\n",
    "        return mcnemar_result.statistic, mcnemar_result.pvalue, contingency_table\n",
    "\n",
    "    # All instances\n",
    "    all_stat, all_pval, all_contingency = perform_test(\n",
    "        true_labels, preds_model_1, preds_model_2\n",
    "    )\n",
    "    results[\"all\"] = (all_stat, all_pval, all_contingency)\n",
    "\n",
    "    # Positive instances\n",
    "    positive_mask = np.where(true_labels == 1)\n",
    "    positive_stat, positive_pval, positive_contingency = perform_test(\n",
    "        true_labels[positive_mask],\n",
    "        preds_model_1[positive_mask],\n",
    "        preds_model_2[positive_mask],\n",
    "    )\n",
    "    results[\"positive\"] = (positive_stat, positive_pval, positive_contingency)\n",
    "\n",
    "    # Negative instances\n",
    "    negative_mask = np.where(true_labels == 0)\n",
    "    negative_stat, negative_pval, negative_contingency = perform_test(\n",
    "        true_labels[negative_mask],\n",
    "        preds_model_1[negative_mask],\n",
    "        preds_model_2[negative_mask],\n",
    "    )\n",
    "    results[\"negative\"] = (negative_stat, negative_pval, negative_contingency)\n",
    "\n",
    "    # Formatting the results for output\n",
    "    formatted_results = {\n",
    "        category: {\n",
    "            \"Statistic\": stat,\n",
    "            \"P-Value\": pval,\n",
    "            \"Contingency Table\": contingency.tolist(),\n",
    "        }\n",
    "        for category, (stat, pval, contingency) in results.items()\n",
    "    }\n",
    "\n",
    "    return formatted_results\n",
    "\n",
    "\n",
    "def load_model(model_config):\n",
    "    \"\"\"\n",
    "    Loads a model based on the provided configuration.\n",
    "\n",
    "    Parameters:\n",
    "    - model_config: A dictionary containing the model's configuration,\n",
    "                    including its feature use configuration and model saving path.\n",
    "\n",
    "    Returns:\n",
    "    - model: The loaded model.\n",
    "    \"\"\"\n",
    "    # Unpack the configuration\n",
    "    feature_use_config = model_config[\"feature_use_config\"]\n",
    "    model_saving_path = model_config[\"model_saving_path\"]\n",
    "\n",
    "    # Load tokenizers (adjust as necessary for your specific setup)\n",
    "    group_desc_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "    )\n",
    "    eligibility_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "    )\n",
    "    smiles_tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "    smiles_tokenizer.add_tokens([\"[PLACEBO]\", \"[NOSMILES]\"])\n",
    "\n",
    "    # Placeholder for NUM_LABELS\n",
    "    NUM_LABELS = 27\n",
    "\n",
    "    # Instantiate model\n",
    "    model = MultilabelModel(\n",
    "        \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        \"DeepChem/ChemBERTa-77M-MLM\",\n",
    "        len(smiles_tokenizer),\n",
    "        NUM_LABELS,\n",
    "        feature_use_config=feature_use_config,\n",
    "    )\n",
    "\n",
    "    # Load the model's state dict\n",
    "    model.load_state_dict(torch.load(model_saving_path))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_baseline_predictions(\n",
    "    train_df: pd.DataFrame, test_df: pd.DataFrame\n",
    ") -> float:\n",
    "\n",
    "    # Extract actual labels from the test set\n",
    "    actuals = test_df.drop(\n",
    "        columns=[\"group_description\", \"eligibility_criteria\", \"smiles\"]\n",
    "    ).values\n",
    "\n",
    "    # Calculate the majority class from the training set\n",
    "    train_df_label = train_df.drop(\n",
    "        columns=[\"group_description\", \"eligibility_criteria\", \"smiles\"]\n",
    "    )\n",
    "    majority_classes = (train_df_label.mean() > 0.5).astype(int).values\n",
    "\n",
    "    # Create predictions using majority classes\n",
    "    predictions_majority_class = np.tile(majority_classes, (actuals.shape[0], 1))\n",
    "\n",
    "    return predictions_majority_class\n",
    "\n",
    "\n",
    "def compute_predictions(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Validate the model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to be evaluated.\n",
    "        dataloader (DataLoader): Dataloader for the validation data.\n",
    "        device (torch.device): Device to run the validation (e.g., 'cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    predictions, actuals = [], []\n",
    "    progress_bar = tqdm(dataloader, leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            logits = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "            predictions.append(logits > 0.5)\n",
    "\n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    return np.vstack(predictions).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7029df37-a5a7-4e94-a50c-9c8aa9db7fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b246f97ed8843d38bca69d9a6e1fdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/y/yazdani0/.local/share/virtualenvs/CTxAI_v2-fPI8ji1Q/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1611190/3809529208.py:130: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  comparison_matrix = comparison_matrix.applymap(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Majority Class Base</th>\n",
       "      <th>Majority Class Augmented</th>\n",
       "      <th>SMILES Base</th>\n",
       "      <th>Group Description &amp; SMILES Base</th>\n",
       "      <th>Eligibility Criteria, Group Description &amp; SMILES Base</th>\n",
       "      <th>Group Description &amp; SMILES Augmented</th>\n",
       "      <th>Eligibility Criteria, Group Description &amp; SMILES Augmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Majority Class Base</th>\n",
       "      <td>1.00e+00</td>\n",
       "      <td>1.00e+00</td>\n",
       "      <td>4.48e-01</td>\n",
       "      <td>1.56e-24</td>\n",
       "      <td>1.05e-58</td>\n",
       "      <td>2.11e-62</td>\n",
       "      <td>1.29e-134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Majority Class Augmented</th>\n",
       "      <td>1.00e+00</td>\n",
       "      <td>1.00e+00</td>\n",
       "      <td>4.48e-01</td>\n",
       "      <td>1.56e-24</td>\n",
       "      <td>1.05e-58</td>\n",
       "      <td>2.11e-62</td>\n",
       "      <td>1.29e-134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMILES Base</th>\n",
       "      <td>4.48e-01</td>\n",
       "      <td>4.48e-01</td>\n",
       "      <td>1.00e+00</td>\n",
       "      <td>1.77e-25</td>\n",
       "      <td>1.54e-64</td>\n",
       "      <td>6.60e-64</td>\n",
       "      <td>3.62e-151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group Description &amp; SMILES Base</th>\n",
       "      <td>1.56e-24</td>\n",
       "      <td>1.56e-24</td>\n",
       "      <td>1.77e-25</td>\n",
       "      <td>1.00e+00</td>\n",
       "      <td>9.59e-23</td>\n",
       "      <td>9.76e-18</td>\n",
       "      <td>2.72e-76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eligibility Criteria, Group Description &amp; SMILES Base</th>\n",
       "      <td>1.05e-58</td>\n",
       "      <td>1.05e-58</td>\n",
       "      <td>1.54e-64</td>\n",
       "      <td>9.59e-23</td>\n",
       "      <td>1.00e+00</td>\n",
       "      <td>7.03e-01</td>\n",
       "      <td>4.50e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group Description &amp; SMILES Augmented</th>\n",
       "      <td>2.11e-62</td>\n",
       "      <td>2.11e-62</td>\n",
       "      <td>6.60e-64</td>\n",
       "      <td>9.76e-18</td>\n",
       "      <td>7.03e-01</td>\n",
       "      <td>1.00e+00</td>\n",
       "      <td>6.26e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eligibility Criteria, Group Description &amp; SMILES Augmented</th>\n",
       "      <td>1.29e-134</td>\n",
       "      <td>1.29e-134</td>\n",
       "      <td>3.62e-151</td>\n",
       "      <td>2.72e-76</td>\n",
       "      <td>4.50e-31</td>\n",
       "      <td>6.26e-30</td>\n",
       "      <td>1.00e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Majority Class Base  \\\n",
       "Majority Class Base                                           1.00e+00   \n",
       "Majority Class Augmented                                      1.00e+00   \n",
       "SMILES Base                                                   4.48e-01   \n",
       "Group Description & SMILES Base                               1.56e-24   \n",
       "Eligibility Criteria, Group Description & SMILE...            1.05e-58   \n",
       "Group Description & SMILES Augmented                          2.11e-62   \n",
       "Eligibility Criteria, Group Description & SMILE...           1.29e-134   \n",
       "\n",
       "                                                   Majority Class Augmented  \\\n",
       "Majority Class Base                                                1.00e+00   \n",
       "Majority Class Augmented                                           1.00e+00   \n",
       "SMILES Base                                                        4.48e-01   \n",
       "Group Description & SMILES Base                                    1.56e-24   \n",
       "Eligibility Criteria, Group Description & SMILE...                 1.05e-58   \n",
       "Group Description & SMILES Augmented                               2.11e-62   \n",
       "Eligibility Criteria, Group Description & SMILE...                1.29e-134   \n",
       "\n",
       "                                                   SMILES Base  \\\n",
       "Majority Class Base                                   4.48e-01   \n",
       "Majority Class Augmented                              4.48e-01   \n",
       "SMILES Base                                           1.00e+00   \n",
       "Group Description & SMILES Base                       1.77e-25   \n",
       "Eligibility Criteria, Group Description & SMILE...    1.54e-64   \n",
       "Group Description & SMILES Augmented                  6.60e-64   \n",
       "Eligibility Criteria, Group Description & SMILE...   3.62e-151   \n",
       "\n",
       "                                                   Group Description & SMILES Base  \\\n",
       "Majority Class Base                                                       1.56e-24   \n",
       "Majority Class Augmented                                                  1.56e-24   \n",
       "SMILES Base                                                               1.77e-25   \n",
       "Group Description & SMILES Base                                           1.00e+00   \n",
       "Eligibility Criteria, Group Description & SMILE...                        9.59e-23   \n",
       "Group Description & SMILES Augmented                                      9.76e-18   \n",
       "Eligibility Criteria, Group Description & SMILE...                        2.72e-76   \n",
       "\n",
       "                                                   Eligibility Criteria, Group Description & SMILES Base  \\\n",
       "Majority Class Base                                                                          1.05e-58      \n",
       "Majority Class Augmented                                                                     1.05e-58      \n",
       "SMILES Base                                                                                  1.54e-64      \n",
       "Group Description & SMILES Base                                                              9.59e-23      \n",
       "Eligibility Criteria, Group Description & SMILE...                                           1.00e+00      \n",
       "Group Description & SMILES Augmented                                                         7.03e-01      \n",
       "Eligibility Criteria, Group Description & SMILE...                                           4.50e-31      \n",
       "\n",
       "                                                   Group Description & SMILES Augmented  \\\n",
       "Majority Class Base                                                            2.11e-62   \n",
       "Majority Class Augmented                                                       2.11e-62   \n",
       "SMILES Base                                                                    6.60e-64   \n",
       "Group Description & SMILES Base                                                9.76e-18   \n",
       "Eligibility Criteria, Group Description & SMILE...                             7.03e-01   \n",
       "Group Description & SMILES Augmented                                           1.00e+00   \n",
       "Eligibility Criteria, Group Description & SMILE...                             6.26e-30   \n",
       "\n",
       "                                                   Eligibility Criteria, Group Description & SMILES Augmented  \n",
       "Majority Class Base                                                                         1.29e-134          \n",
       "Majority Class Augmented                                                                    1.29e-134          \n",
       "SMILES Base                                                                                 3.62e-151          \n",
       "Group Description & SMILES Base                                                              2.72e-76          \n",
       "Eligibility Criteria, Group Description & SMILE...                                           4.50e-31          \n",
       "Group Description & SMILES Augmented                                                         6.26e-30          \n",
       "Eligibility Criteria, Group Description & SMILE...                                           1.00e+00          "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "train_df_base, val_df, test_df = load_trial_data(\n",
    "    \"../data/classification/smiles/train_base\"\n",
    ")\n",
    "train_df_augmented, _, _ = load_trial_data(\n",
    "    \"../data/classification/smiles/train_augmented\"\n",
    ")\n",
    "\n",
    "# Configuration for each model\n",
    "model_configs = [\n",
    "    {\n",
    "        \"feature_use_config\": {\n",
    "            \"group_desc\": False,\n",
    "            \"eligibility\": False,\n",
    "            \"smiles\": True,\n",
    "        },\n",
    "        \"model_saving_path\": \"../models/smiles/train_base/smiles_only/model.pt\",\n",
    "    },\n",
    "    {\n",
    "        \"feature_use_config\": {\n",
    "            \"group_desc\": True,\n",
    "            \"eligibility\": False,\n",
    "            \"smiles\": True,\n",
    "        },\n",
    "        \"model_saving_path\": \"../models/smiles/train_base/smiles_group_desc/model.pt\",\n",
    "    },\n",
    "    {\n",
    "        \"feature_use_config\": {\"group_desc\": True, \"eligibility\": True, \"smiles\": True},\n",
    "        \"model_saving_path\": \"../models/smiles/train_base/all/model.pt\",\n",
    "    },\n",
    "    {\n",
    "        \"feature_use_config\": {\n",
    "            \"group_desc\": True,\n",
    "            \"eligibility\": False,\n",
    "            \"smiles\": True,\n",
    "        },\n",
    "        \"model_saving_path\": \"../models/smiles/train_augmented/smiles_group_desc/model.pt\",\n",
    "    },\n",
    "    {\n",
    "        \"feature_use_config\": {\"group_desc\": True, \"eligibility\": True, \"smiles\": True},\n",
    "        \"model_saving_path\": \"../models/smiles/train_augmented/all/model.pt\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Load all models\n",
    "models = [load_model(config) for config in tqdm(model_configs)]\n",
    "\n",
    "group_desc_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    ")\n",
    "eligibility_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    ")\n",
    "smiles_tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "smiles_tokenizer.add_tokens([\"[PLACEBO]\", \"[NOSMILES]\"])\n",
    "\n",
    "# Test datasets\n",
    "test_dataset = CustomDataset(\n",
    "    test_df,\n",
    "    group_desc_tokenizer,\n",
    "    eligibility_tokenizer,\n",
    "    smiles_tokenizer,\n",
    "    text_max_len=512,\n",
    "    smiles_max_len=512,\n",
    ")\n",
    "\n",
    "# Test dataloaders\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# Use .flatten() to micro-average\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "true_labels = test_df.drop(\n",
    "    columns=[\"group_description\", \"eligibility_criteria\", \"smiles\"]\n",
    ").values.flatten()\n",
    "baseline_base_preds = compute_baseline_predictions(train_df_base, test_df).flatten()\n",
    "baseline_augmented_preds = compute_baseline_predictions(\n",
    "    train_df_augmented, test_df\n",
    ").flatten()\n",
    "models_0_preds = compute_predictions(models[0], test_dataloader, device).flatten()\n",
    "models_1_preds = compute_predictions(models[1], test_dataloader, device).flatten()\n",
    "models_2_preds = compute_predictions(models[2], test_dataloader, device).flatten()\n",
    "models_3_preds = compute_predictions(models[3], test_dataloader, device).flatten()\n",
    "models_4_preds = compute_predictions(models[4], test_dataloader, device).flatten()\n",
    "\n",
    "models_predictions = {\n",
    "    \"Majority Class Base\": baseline_base_preds,\n",
    "    \"Majority Class Augmented\": baseline_augmented_preds,\n",
    "    \"SMILES Base\": models_0_preds,\n",
    "    \"Group Description & SMILES Base\": models_1_preds,\n",
    "    \"Eligibility Criteria, Group Description & SMILES Base\": models_2_preds,\n",
    "    \"Group Description & SMILES Augmented\": models_3_preds,\n",
    "    \"Eligibility Criteria, Group Description & SMILES Augmented\": models_4_preds,\n",
    "}\n",
    "\n",
    "# Perform pairwise comparisons\n",
    "comparison_results = {}\n",
    "\n",
    "for (model_1_name, preds_model_1), (\n",
    "    model_2_name,\n",
    "    preds_model_2,\n",
    ") in itertools.combinations(models_predictions.items(), 2):\n",
    "    results = mcnemar_test(true_labels, preds_model_1, preds_model_2, exact=True)\n",
    "    all_instance_results = results[\"all\"]\n",
    "    statistic, pvalue = (\n",
    "        all_instance_results[\"Statistic\"],\n",
    "        all_instance_results[\"P-Value\"],\n",
    "    )\n",
    "    comparison_name = f\"{model_1_name} vs {model_2_name}\"\n",
    "    comparison_results[comparison_name] = {\"Statistic\": statistic, \"P-Value\": pvalue}\n",
    "\n",
    "original_alpha = 0.05\n",
    "number_of_comparisons = len(comparison_results)\n",
    "adjusted_alpha = original_alpha / number_of_comparisons  # Bonferroni correction\n",
    "\n",
    "model_names = list(models_predictions.keys())\n",
    "comparison_matrix = pd.DataFrame(index=model_names, columns=model_names)\n",
    "\n",
    "# Populate the DataFrame with p-values\n",
    "for comparison, result in comparison_results.items():\n",
    "    model_1_name, model_2_name = comparison.split(\" vs \")\n",
    "    pvalue = result[\"P-Value\"]\n",
    "    # Fill symmetric cells with the p-value\n",
    "    comparison_matrix.loc[model_1_name, model_2_name] = pvalue\n",
    "    comparison_matrix.loc[model_2_name, model_1_name] = pvalue\n",
    "\n",
    "# Fill the diagonal with 1 since a model compared with itself is not significant\n",
    "np.fill_diagonal(comparison_matrix.values, 1)\n",
    "comparison_matrix = comparison_matrix.applymap(\n",
    "    lambda x: \"{:.2e}\".format(float(x)) if isinstance(x, (int, float)) else x\n",
    ")\n",
    "comparison_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctxaiv2",
   "language": "python",
   "name": "ctxaiv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
